{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:07:57.530057: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-12 16:07:57.543775: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741792077.557887   26493 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741792077.561719   26493 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-12 16:07:57.577263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "from experiments.data.data import get_data, UNI_DATASETS, MULTI_DATASETS\n",
    "from experiments.models.classifier import GradientInceptionTimeClassifier\n",
    "from mascots.explainer.borf import BorfExplainer\n",
    "from mascots.explainer.pipeline import get_borf_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-12 16:12:31.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='TwoLeadECG'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:12:32.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 4\u001b[0m\n",
      "\u001b[32m2025-03-12 16:12:32.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (82) than timepoints (1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 1, 82)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 98.3 K | train\n",
      "----------------------------------------------\n",
      "98.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.3 K    Total params\n",
      "0.393     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:12:40.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:12:40.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mTwoLeadECG: {'accuracy': np.float64(1.0), 'cross-entropy': np.float32(-0.033395935), 'mse': np.float32(0.005726279), 'r2': 0.9770127534866333}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:12:40.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='GunPoint'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:12:41.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 5\u001b[0m\n",
      "\u001b[32m2025-03-12 16:12:41.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 128, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1, 150)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (150) than timepoints (1)\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 122 K  | train\n",
      "----------------------------------------------\n",
      "122 K     Trainable params\n",
      "0         Non-trainable params\n",
      "122 K     Total params\n",
      "0.491     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:12:47.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:12:47.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mGunPoint: {'accuracy': np.float64(1.0), 'cross-entropy': np.float32(-0.023027062), 'mse': np.float32(0.0044845883), 'r2': 0.982015073299408}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:12:47.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='Earthquakes'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:12:48.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 7\u001b[0m\n",
      "\u001b[32m2025-03-12 16:12:48.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 128, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 256, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 512, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 1, 512)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (512) than timepoints (1)\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 133 K  | train\n",
      "----------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.532     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[32m2025-03-12 16:13:54.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:13:54.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mEarthquakes: {'accuracy': np.float64(0.8167701863354038), 'cross-entropy': np.float32(-0.5487962), 'mse': np.float32(0.18148986), 'r2': -0.23454809188842773}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:13:54.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='Coffee'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:13:55.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 6\u001b[0m\n",
      "\u001b[32m2025-03-12 16:13:55.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 128, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 256, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (286) than timepoints (1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 1, 286)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 126 K  | train\n",
      "----------------------------------------------\n",
      "126 K     Trainable params\n",
      "0         Non-trainable params\n",
      "126 K     Total params\n",
      "0.506     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:14:00.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:00.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mCoffee: {'accuracy': np.float64(1.0), 'cross-entropy': np.float32(-0.011938408), 'mse': np.float32(0.00056080066), 'r2': 0.9977453947067261}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:00.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='Wine'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:14:01.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 5\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:01.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 128, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 1, 234)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (234) than timepoints (1)\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 103 K  | train\n",
      "----------------------------------------------\n",
      "103 K     Trainable params\n",
      "0         Non-trainable params\n",
      "103 K     Total params\n",
      "0.416     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:14:06.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:06.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mWine: {'accuracy': np.float64(0.8421052631578947), 'cross-entropy': np.float32(-0.3450611), 'mse': np.float32(0.0022980822), 'r2': -0.12513494491577148}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:06.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='ItalyPowerDemand'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:14:07.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 2\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:07.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (24) than timepoints (1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 1, 24)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 88.8 K | train\n",
      "----------------------------------------------\n",
      "88.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "88.8 K    Total params\n",
      "0.355     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:14:25.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:25.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mItalyPowerDemand: {'accuracy': np.float64(0.9104477611940298), 'cross-entropy': np.float32(-0.13647155), 'mse': np.float32(0.07127551), 'r2': 0.7080247402191162}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:25.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='CBF'\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:25.579\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[31m\u001b[1mCBF failed\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:25.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='Herring'\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:26.190\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[31m\u001b[1mHerring failed\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:26.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='Fish'\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:26.827\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[31m\u001b[1mFish failed\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:26.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='ArticularyWordRecognition'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:14:27.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 5\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:27.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 128, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 9, 144)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (144) than timepoints (9)\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 701 K  | train\n",
      "----------------------------------------------\n",
      "701 K     Trainable params\n",
      "0         Non-trainable params\n",
      "701 K     Total params\n",
      "2.807     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[32m2025-03-12 16:14:56.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:56.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mArticularyWordRecognition: {'accuracy': np.float64(1.0), 'cross-entropy': np.float32(-0.073822185), 'mse': np.float32(0.0019403846), 'r2': -5.902790546417236}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:56.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='BasicMotions'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:14:57.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 4\u001b[0m\n",
      "\u001b[32m2025-03-12 16:14:57.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (100) than timepoints (6)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 6, 100)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 346 K  | train\n",
      "----------------------------------------------\n",
      "346 K     Trainable params\n",
      "0         Non-trainable params\n",
      "346 K     Total params\n",
      "1.384     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:15:02.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:02.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mBasicMotions: {'accuracy': np.float64(1.0), 'cross-entropy': np.float32(-0.035366528), 'mse': np.float32(0.0012702232), 'r2': -1.190284013748169}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:02.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='Cricket'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:15:03.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 8\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:03.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 128, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 256, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 512, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 1024, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 6, 1197)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (1197) than timepoints (6)\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 747 K  | train\n",
      "----------------------------------------------\n",
      "747 K     Trainable params\n",
      "0         Non-trainable params\n",
      "747 K     Total params\n",
      "2.991     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[32m2025-03-12 16:15:22.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:22.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mCricket: {'accuracy': np.float64(0.6018518518518519), 'cross-entropy': np.float32(-0.36680815), 'mse': np.float32(0.048058), 'r2': -1.1789164543151855}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:22.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='Epilepsy'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:15:23.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 5\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:23.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 128, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (206) than timepoints (3)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 3, 206)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 282 K  | train\n",
      "----------------------------------------------\n",
      "282 K     Trainable params\n",
      "0         Non-trainable params\n",
      "282 K     Total params\n",
      "1.132     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[32m2025-03-12 16:15:34.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:34.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mEpilepsy: {'accuracy': np.float64(0.8978102189781022), 'cross-entropy': np.float32(-0.50213325), 'mse': np.float32(0.045896918), 'r2': -0.8397647142410278}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:34.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='ERing'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:15:35.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 4\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:35.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (65) than timepoints (4)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 4, 65)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 190 K  | train\n",
      "----------------------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.764     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:15:41.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:41.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mERing: {'accuracy': np.float64(1.0), 'cross-entropy': np.float32(-0.4011731), 'mse': np.float32(0.032333616), 'r2': -11.065766334533691}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:41.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='NATOPS'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:15:42.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 3\u001b[0m\n",
      "\u001b[32m2025-03-12 16:15:42.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 24, 51)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (51) than timepoints (24)\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 1.1 M  | train\n",
      "----------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.361     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[32m2025-03-12 16:16:19.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:16:19.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mNATOPS: {'accuracy': np.float64(0.6777777777777778), 'cross-entropy': np.float32(-0.20336483), 'mse': np.float32(0.005207696), 'r2': -2463133.75}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:16:19.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='RacketSports'\u001b[0m\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "\u001b[32m2025-03-12 16:16:20.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 2\u001b[0m\n",
      "\u001b[32m2025-03-12 16:16:20.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (30) than timepoints (6)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 6, 30)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 241 K  | train\n",
      "----------------------------------------------\n",
      "241 K     Trainable params\n",
      "0         Non-trainable params\n",
      "241 K     Total params\n",
      "0.966     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[32m2025-03-12 16:16:41.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n",
      "\u001b[32m2025-03-12 16:16:41.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mRacketSports: {'accuracy': np.float64(0.6423841059602649), 'cross-entropy': np.float32(-0.24989273), 'mse': np.float32(0.07071219), 'r2': -39.70420455932617}\u001b[0m\n",
      "\u001b[32m2025-03-12 16:16:41.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mdata_name='ECG200'\u001b[0m\n",
      "\u001b[32m2025-03-12 16:16:42.172\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[31m\u001b[1mECG200 failed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "\n",
    "for data_name in UNI_DATASETS + MULTI_DATASETS:\n",
    "    logger.info(f'{data_name=}')\n",
    "    try:\n",
    "        model_path = Path(\n",
    "                f\"experiments/out/models/classifier/{data_name}/{tf.__version__}/model_no_padding\"\n",
    "            )\n",
    "\n",
    "        X_train, y_train, X_test, y_test = get_data(data_name)\n",
    "        blackbox = GradientInceptionTimeClassifier.load(model_path)\n",
    "        \n",
    "        explainer = BorfExplainer(blackbox.predict_cls, blackbox.predict, borf_config=get_borf_config(X_train.shape))\n",
    "        scores = explainer.build(np.array(X_train),\n",
    "                                    'shap',\n",
    "                                    attribution_args={'mode': 'normal'})\n",
    "        \n",
    "        res[data_name] = scores\n",
    "        logger.info(f'{data_name}: {scores}')\n",
    "        \n",
    "    except Exception:\n",
    "        logger.error(f'{data_name} failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TwoLeadECG': {'accuracy': np.float64(1.0),\n",
       "  'cross-entropy': np.float32(-0.033395935),\n",
       "  'mse': np.float32(0.005726279),\n",
       "  'r2': 0.9770127534866333},\n",
       " 'GunPoint': {'accuracy': np.float64(1.0),\n",
       "  'cross-entropy': np.float32(-0.023027062),\n",
       "  'mse': np.float32(0.0044845883),\n",
       "  'r2': 0.982015073299408},\n",
       " 'Earthquakes': {'accuracy': np.float64(0.8167701863354038),\n",
       "  'cross-entropy': np.float32(-0.5487962),\n",
       "  'mse': np.float32(0.18148986),\n",
       "  'r2': -0.23454809188842773},\n",
       " 'Coffee': {'accuracy': np.float64(1.0),\n",
       "  'cross-entropy': np.float32(-0.011938408),\n",
       "  'mse': np.float32(0.00056080066),\n",
       "  'r2': 0.9977453947067261},\n",
       " 'Wine': {'accuracy': np.float64(0.8421052631578947),\n",
       "  'cross-entropy': np.float32(-0.3450611),\n",
       "  'mse': np.float32(0.0022980822),\n",
       "  'r2': -0.12513494491577148},\n",
       " 'ItalyPowerDemand': {'accuracy': np.float64(0.9104477611940298),\n",
       "  'cross-entropy': np.float32(-0.13647155),\n",
       "  'mse': np.float32(0.07127551),\n",
       "  'r2': 0.7080247402191162},\n",
       " 'ArticularyWordRecognition': {'accuracy': np.float64(1.0),\n",
       "  'cross-entropy': np.float32(-0.073822185),\n",
       "  'mse': np.float32(0.0019403846),\n",
       "  'r2': -5.902790546417236},\n",
       " 'BasicMotions': {'accuracy': np.float64(1.0),\n",
       "  'cross-entropy': np.float32(-0.035366528),\n",
       "  'mse': np.float32(0.0012702232),\n",
       "  'r2': -1.190284013748169},\n",
       " 'Cricket': {'accuracy': np.float64(0.6018518518518519),\n",
       "  'cross-entropy': np.float32(-0.36680815),\n",
       "  'mse': np.float32(0.048058),\n",
       "  'r2': -1.1789164543151855},\n",
       " 'Epilepsy': {'accuracy': np.float64(0.8978102189781022),\n",
       "  'cross-entropy': np.float32(-0.50213325),\n",
       "  'mse': np.float32(0.045896918),\n",
       "  'r2': -0.8397647142410278},\n",
       " 'ERing': {'accuracy': np.float64(1.0),\n",
       "  'cross-entropy': np.float32(-0.4011731),\n",
       "  'mse': np.float32(0.032333616),\n",
       "  'r2': -11.065766334533691},\n",
       " 'NATOPS': {'accuracy': np.float64(0.6777777777777778),\n",
       "  'cross-entropy': np.float32(-0.20336483),\n",
       "  'mse': np.float32(0.005207696),\n",
       "  'r2': -2463133.75},\n",
       " 'RacketSports': {'accuracy': np.float64(0.6423841059602649),\n",
       "  'cross-entropy': np.float32(-0.24989273),\n",
       "  'mse': np.float32(0.07071219),\n",
       "  'r2': -39.70420455932617}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-12 16:11:06.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mBorf #configs: 4\u001b[0m\n",
      "\u001b[32m2025-03-12 16:11:06.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mBorf #configs: [{'window_size': 8, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 16, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 32, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}, {'window_size': 64, 'stride': 1, 'dilation': 1, 'word_length': 4, 'alphabet_size': 3}]\u001b[0m\n",
      "/home/dawid/mi2/pineberry/code/borf-xai/experiments/models/classifier.py:86: UserWarning: There are more features (82) than timepoints (1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 1, 82)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 98.3 K | train\n",
      "----------------------------------------------\n",
      "98.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.3 K    Total params\n",
      "0.393     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/dawid/miniconda3/envs/borf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[32m2025-03-12 16:11:17.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mborf.explainer.borf\u001b[0m:\u001b[36m_map_borf_features\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mcreate inner representation\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "explainer = BorfExplainer(blackbox.predict_cls, blackbox.predict, borf_config=get_borf_config(X_train.shape))\n",
    "scores = explainer.build(np.array(X_train),\n",
    "                            'shap',\n",
    "                            attribution_args={'mode': 'normal'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
